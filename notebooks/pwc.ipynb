{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9963e52a-f0bd-45e6-8486-ed1daf6eea27",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import successful\n"
     ]
    }
   ],
   "source": [
    "from neo4j import GraphDatabase\n",
    "import os\n",
    "import pandas as pd\n",
    "print(\"Import successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fb99faa-1cc5-414b-b3ab-a20463c99983",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "URI = os.environ[\"NEO4J_URI\"]\n",
    "USER=os.environ[\"NEO4J_USER_NAME\"]\n",
    "PASSWORD=os.environ[\"NEO4J_PASSWD\"]\n",
    "AUTH = (os.environ[\"NEO4J_USER_NAME\"], os.environ[\"NEO4J_PASSWD\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d7419f0-36da-48b4-8e6b-f2a3b157e088",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Neo4J connect and Query Boilerplate\n",
    "\n",
    "class Neo4jConnection:\n",
    "    \n",
    "    def __init__(self, uri, user, pwd):\n",
    "        self.__uri = uri\n",
    "        self.__user = user\n",
    "        self.__pwd = pwd\n",
    "        self.__driver = None\n",
    "        try:\n",
    "            self.__driver = GraphDatabase.driver(self.__uri, auth=(self.__user, self.__pwd))\n",
    "        except Exception as e:\n",
    "            print(\"Failed to create the driver:\", e)\n",
    "        \n",
    "    def close(self):\n",
    "        if self.__driver is not None:\n",
    "            self.__driver.close()\n",
    "        \n",
    "    def query(self, query, parameters=None, db=None):\n",
    "        assert self.__driver is not None, \"Driver not initialized!\"\n",
    "        session = None\n",
    "        response = None\n",
    "        try: \n",
    "            session = self.__driver.session(database=db) if db is not None else self.__driver.session() \n",
    "            #response = (session.run(query, parameters))\n",
    "            response = list(session.run(query, parameters))\n",
    "        except Exception as e:\n",
    "            print(\"Query failed:\", e)\n",
    "        finally: \n",
    "            if session is not None:\n",
    "                session.close()\n",
    "        \n",
    "        #return pd.DataFrame([r.values() for r in response], columns=response.keys())\n",
    "        return response\n",
    "    \n",
    "    def multi_query(self, multi_line_query, parameters=None, db=None):\n",
    "        for li in multi_line_query.splitlines():\n",
    "                print(li)\n",
    "                result=self.query(li, parameters=None, db=None)\n",
    "                print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79d802a5-b575-45f7-841f-8f2c07ddbb1f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Record count(n)=17474>]\n"
     ]
    }
   ],
   "source": [
    "#Make a default connection and it should return `[<Record count(n)=0>]`\n",
    "conn = Neo4jConnection(uri=URI, \n",
    "                       user=USER,              \n",
    "                       pwd=PASSWORD)\n",
    "\n",
    "#if db is empty, then seed with init values \n",
    "res=conn.query('MATCH (n) RETURN count(n)')\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b6dad656-0e61-4ebd-84d1-1fb87ac11fab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Record count(n)=0>]\n"
     ]
    }
   ],
   "source": [
    "#Optional\n",
    "#Clean Neo4J DB, query the DB and print the result\n",
    "\n",
    "res=conn.query(\"\"\"\n",
    "MATCH (n) DETACH DELETE n;\n",
    "\"\"\")\n",
    "\n",
    "res=conn.query('MATCH (n) RETURN count(n)')\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224dafbb-d20b-4c1f-b0ba-ca0f92c1a6b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Queries\n",
    "\n",
    "# Load Pipeline info. There are more info on the pipeline. Not loading them at this moment. Will be laoded in the final version.\n",
    "# Data is available in the CSV. Just need to be included in the query\n",
    "load_pipeline = \"\"\"LOAD CSV WITH HEADERS FROM 'file:///small-papers.csv' AS row\n",
    "MERGE (pipeline:Pipeline {pipelineID: row.id})\n",
    "ON CREATE SET pipeline.Title = row.title, pipeline.urlAbs = row.url_abs, pipeline.urlPdf = row.url_pdf, pipeline.gitRepo = row.git_repos;\"\"\"\n",
    "\n",
    "# Load the task database\n",
    "load_tasks = \"\"\"LOAD CSV WITH HEADERS FROM 'file:///pwc-tasks.csv' AS row\n",
    "MERGE (task:Task {taskID: row.id}) \n",
    "ON CREATE SET task.taskName = row.name, task.taskDesc = row.description, task.category = row.category, task.modality = row.modality;\"\"\"\n",
    "\n",
    "# Load Datasets\n",
    "load_datasets = \"\"\"LOAD CSV WITH HEADERS FROM 'file:///pwc-datasets.csv' AS row\n",
    "MERGE (dataset:Dataset {datasetID: row.id})\n",
    "ON CREATE SET dataset.datasetName = row.name, dataset.datasetFullName = row.full_name, dataset.datasetDesc = row.description, dataset.url = row.url;\"\"\"\n",
    "\n",
    "# Load Models/Methods\n",
    "load_methods = \"\"\"LOAD CSV WITH HEADERS FROM 'file:///pwc-methods.csv' AS row\n",
    "MERGE (method:Method {methodID: row.id})\n",
    "ON CREATE SET method.methodName = row.name, method.methodFullName = row.full_name, method.modelDesc = row.description;\"\"\"\n",
    "\n",
    "# Load Evaluations\n",
    "load_evals = \"\"\"LOAD CSV WITH HEADERS FROM 'file:///pwc-evaluations.csv' AS row\n",
    "MERGE (eval:Evaluation {evalID: row.id})\n",
    "ON CREATE SET eval.evalName = row.name, eval.evalDesc = row.description;\"\"\"\n",
    "\n",
    "# Load Results\n",
    "load_results = \"\"\"LOAD CSV WITH HEADERS FROM 'file:///pwc-results.csv' AS row\n",
    "MERGE (result:Result {resultID: row.id})\n",
    "ON CREATE SET result.resultMetrics = row.metrics, result.bestRank = row.best_rank, result.methodology = row.methodology, \n",
    "result.bestMetric = row.best_metric;\"\"\"\n",
    "\n",
    "\n",
    "# Before relationships, need to construct a constraint\n",
    "# constraint = \"\"\"\n",
    "# CREATE INDEX paper_id FOR (pipeline:Pipeline) ON (pipeline.pipelineID);\n",
    "# CREATE INDEX task_id FOR (task:Task) ON (task.taskID);\n",
    "# CREATE INDEX dataset_id FOR (dataset:Dataset) ON (dataset.datasetID);\n",
    "# CREATE INDEX method_id FOR (method:Method) ON (method.methodID);\n",
    "# CREATE INDEX eval_id FOR (eval:Evaluation) ON (eval.evalID);\n",
    "# CREATE INDEX result_id FOR (result:Result) ON (result.resultID);\n",
    "# CREATE CONSTRAINT pipeline_id FOR (pipeline:Pipeline) REQUIRE pipeline.pipelineID IS UNIQUE;\n",
    "# CALL db.awaitIndexes();\n",
    "# \"\"\"\n",
    "\n",
    "constraint = \"\"\"\n",
    "CREATE INDEX dataset_id FOR (dataset:Dataset) ON (dataset.datasetID);\n",
    "CREATE CONSTRAINT eval_id FOR (eval:Evaluation) REQUIRE eval.evalID IS UNIQUE;\n",
    "\"\"\"\n",
    "\n",
    "call = \"\"\"CALL db.indexes();\"\"\"\n",
    "\n",
    "# Loading Relationships\n",
    "# Paper-Dataset relationship\n",
    "rel_pipeline_dataset = \"\"\"LOAD CSV WITH HEADERS FROM 'file:///relations/eval_dataset.csv' AS row\n",
    "MATCH (eval:Evaluation {evalID: row.eval_id})\n",
    "MATCH (dataset:Dataset {datasetID: row.dataset_id})\n",
    "MERGE (eval)-[pd:evaluatedON]->(dataset);\"\"\"\n",
    "#ON CREATE SET op.unitPrice = toFloat(row.UnitPrice), op.quantity = toFloat(row.Quantity);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8a0ab1-90d6-4d8c-88d2-676fbc5adf8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# res_pipeline=conn.query(load_pipeline)\n",
    "# res_task=conn.query(load_tasks)\n",
    "res_datasets=conn.query(load_datasets)\n",
    "# res_methods=conn.query(load_methods)\n",
    "res_evals=conn.query(load_evals)\n",
    "# res_results=conn.query(load_results)\n",
    "print(\"Nodes added\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "675eabfd-3383-48fc-b22d-1048213bd377",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Record p=<Node element_id='4' labels=frozenset({'Task'}) properties={'modality': '[]', 'category': '[]', 'taskID': 'task'}>>, <Record p=<Node element_id='5' labels=frozenset({'Task'}) properties={'taskName': '2048', 'modality': '[]', 'category': '[]', 'taskID': '2048'}>>, <Record p=<Node element_id='6' labels=frozenset({'Task'}) properties={'taskName': '2D Classification', 'modality': \"['2d']\", 'category': \"['classification']\", 'taskID': '2d-classification'}>>, <Record p=<Node element_id='7' labels=frozenset({'Task'}) properties={'taskDesc': 'What is Human Pose Estimation?\\r\\nHuman pose estimation is the process of estimating the configuration of the body (pose) from a single, typically monocular, image. Background. Human pose estimation is one of the key problems in computer vision that has been studied for well over 15 years.  The reason for its importance is the\\r\\nabundance of applications that can benefit from such a technology. For example,\\r\\nhuman pose estimation allows for higher-level reasoning in the context of human-computer interaction and activity recognition; it is also one of the basic building blocks for marker-less motion capture (MoCap) technology. MoCap technology is useful for applications ranging from character animation to clinical analysis of gait pathologies.', 'taskName': '2D Human Pose Estimation', 'modality': \"['2d']\", 'category': \"['estimation']\", 'taskID': '2d-human-pose-estimation'}>>, <Record p=<Node element_id='8' labels=frozenset({'Task'}) properties={'taskName': '2D object detection', 'modality': \"['2d']\", 'category': \"['detection']\", 'taskID': '2d-object-detection'}>>]\n"
     ]
    }
   ],
   "source": [
    "# res=conn.query('MATCH (p:Task) return p LIMIT 5;')\n",
    "# relationship_query = conn.query('MATCH path = (dataset:Dataset {datasetID: \"mnist\"})<-[pd:evaluatedON]-(eval)\n",
    "# RETURN path\n",
    "# LIMIT 25;')\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05294f9-6826-4b22-9f2d-9b97cc860d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading rdf. Src - https://neo4j.com/labs/neosemantics/tutorial/\n",
    "\n",
    "# MUST - Create a constraint to generate and unique URI to the nodes\n",
    "constraint = \"\"\"CREATE CONSTRAINT n10s_unique_uri ON (r:Resource)\n",
    "ASSERT r.uri IS UNIQUE\"\"\"\n",
    "\n",
    "# To drop the constraint - DROP CONSTRAINT ON (r:Resource) ASSERT r.uri IS UNIQUE\n",
    "\n",
    "# Add graph Config\n",
    " graph_config = \"\"\"CALL n10s.graphconfig.init({\n",
    "  handleVocabUris: 'MAP'\n",
    "})\"\"\"\n",
    "\n",
    "load_rdf = \"\"\"CALL n10s.rdf.preview.fetch(\n",
    "  'https://raw.githubusercontent.com/ML-Schema/core/master/MLSchema.ttl',\n",
    "  'Turtle'\n",
    ")\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
